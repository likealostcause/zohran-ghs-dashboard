{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "874d26d5",
   "metadata": {},
   "source": [
    "# Join Processed Data to School Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a54a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "schools = gpd.read_file('../data/processed_data/school_points_with_lcgms.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af025da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "schools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde7687f",
   "metadata": {},
   "source": [
    "## Join Boroughs\n",
    "\n",
    "Lost borough names at an earlier stage so just going to bring them back in here via spatial join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18243f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "boroughs = gpd.read_file('../data/raw_data/NYC Planning/nybb_25c/nybb.shp')[['BoroName', 'geometry']].to_crs(schools.crs)\n",
    "master_schools = gpd.sjoin(schools, boroughs, how='left', predicate='within').drop(columns=['index_right'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c87b3e3",
   "metadata": {},
   "source": [
    "## Join DACs to Schools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6319efba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dacs = gpd.read_file('../data/processed_data/dac_nyc_lite.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752efe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that there aren't any public schools exactly on the border of a DAC\n",
    "assert schools.geometry.apply(dacs.union_all().covers).sum() == schools.geometry.within(dacs.union_all()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc0cdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_schools = gpd.sjoin(schools, dacs, how='left', predicate='within')\n",
    "master_schools.drop(columns=['index_right', 'county', 'geoid'], inplace=True)\n",
    "master_schools['dac_designation'] = master_schools['dac_designation'].fillna(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61f03d2",
   "metadata": {},
   "source": [
    "## Join Election Results to Schools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a9ac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_results = gpd.read_file('../data/processed_data/zohran_first_round_frac.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cadcb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproject to planar CRS Web Mercator (EPSG:3857) for accurate distance calculations\n",
    "master_schools_og_crs = master_schools.crs\n",
    "primary_results_og_crs = primary_results.crs\n",
    "master_schools = master_schools.to_crs('EPSG:3857')\n",
    "primary_results = primary_results.to_crs('EPSG:3857')\n",
    "\n",
    "# First do regular spatial join\n",
    "master_schools = gpd.sjoin(master_schools, primary_results, how='left', predicate='within').drop(columns=['index_right'])\n",
    "\n",
    "# Find unmatched schools\n",
    "unmatched_mask = master_schools['ZohranFirstRoundFrac'].isna()\n",
    "unmatched_schools = master_schools[unmatched_mask].copy()\n",
    "\n",
    "print(f\"Found {unmatched_mask.sum()} schools without polygon matches, using nearest neighbor...\")\n",
    "\n",
    "# Use sjoin_nearest for unmatched schools\n",
    "nearest_join = gpd.tools.sjoin_nearest(unmatched_schools.drop(columns='ZohranFirstRoundFrac'), primary_results, how='left')\n",
    "master_schools.loc[unmatched_mask, 'ZohranFirstRoundFrac'] = nearest_join['ZohranFirstRoundFrac'].values\n",
    "\n",
    "assert not master_schools['ZohranFirstRoundFrac'].isna().any()\n",
    "\n",
    "# Reproject back to original CRS\n",
    "master_schools = master_schools.to_crs(master_schools_og_crs)\n",
    "primary_results = primary_results.to_crs(primary_results_og_crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0dda7f",
   "metadata": {},
   "source": [
    "## Join A/C Data\n",
    "\n",
    "Data scraped from NYC Schools individual webpages for each building code in LCGMS data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a35bcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_ac_summary = pd.read_csv('../data/processed_data/no_ac_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ad1b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are missing 185 unique building codes from our scraped A/C data\n",
    "ct_bldg_codes_missing_from_ac_data = master_schools['Bldg_Code'].nunique() - no_ac_summary['BuildingCode'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5268442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_schools = master_schools.merge(no_ac_summary, left_on='Bldg_Code', right_on='BuildingCode', how='left').drop(columns=['BuildingCode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19eed9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show that all the building codes we had from AC data successfully joined\n",
    "assert master_schools.drop_duplicates(subset=['Bldg_Code'])['CLS_No_AC'].isna().sum() == ct_bldg_codes_missing_from_ac_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb6d4e7",
   "metadata": {},
   "source": [
    "## Join Ventilation Data\n",
    "\n",
    "Data scraped from NYC Schools individual webpages for each building code in LCGMS data.\n",
    "\n",
    "The `No_VT` columns here indicate that there is at least one element of *mechanical* ventilation that is non-existent. This is NOT the definition of functioning ventilation that NYC Schools website uses -- theirs allows for the existence of windows to count as adequate ventilation. However, due to the increasing frequency of air pollution from wildfire smoke and the necessity of HEPA filters in classrooms, we are instead looking at whether BOTH a supply and exhaust fan exist in a room. If a room is missing EITHER a supply fan OR an exhaust fan, it is counted in the `No_VT` column for that room type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bf6800",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_ventilation_summary = pd.read_csv('../data/processed_data/missing_ventilation_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfc2f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are missing 198 unique building codes from our scraped ventilation data\n",
    "ct_bldg_codes_missing_from_vent_data = master_schools['Bldg_Code'].nunique() - missing_ventilation_summary['BuildingCode'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de24e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_schools = master_schools.merge(missing_ventilation_summary, left_on='Bldg_Code', right_on='BuildingCode', how='left').drop(columns=['BuildingCode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac81341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show that all the building codes we had from ventilation data successfully joined\n",
    "assert master_schools.drop_duplicates(subset=['Bldg_Code'])['CLS_No_VT'].isna().sum() == ct_bldg_codes_missing_from_vent_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6b5e85",
   "metadata": {},
   "source": [
    "## Join Building Capacity + Utilization Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9e4497",
   "metadata": {},
   "outputs": [],
   "source": [
    "bldg_capacity_utilization = pd.read_csv('../data/processed_data/bldg_capacity_utilization.csv')\n",
    "bldg_capacity_utilization.rename(columns={'Bldg ID': 'Bldg_Code', 'Bldg Enroll': 'Bldg_Enroll', 'Target Bldg Cap': 'Bldg_Cap', 'Target Bldg Util': 'Bldg_Util', 'Data As Of': 'Util_As_Of'}, inplace=True)\n",
    "bldg_capacity_utilization['Util_As_Of'] = pd.to_datetime(bldg_capacity_utilization['Util_As_Of'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20864d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_schools = master_schools.merge(\n",
    "    bldg_capacity_utilization[['Bldg_Code', 'Bldg_Enroll', 'Bldg_Cap', 'Bldg_Util', 'Util_As_Of']],\n",
    "    on='Bldg_Code',\n",
    "    how='left'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557056bf",
   "metadata": {},
   "source": [
    "## Join IBO School Barriers Data\n",
    "\n",
    "This dataset has capacity/utilization, percentage of space with A/C, building accessibility, and some other really cool fields, and they're all already joined to location code. So hoping this will join well\n",
    "\n",
    "NOTE: looks like IBO only included schools that were in all of the datasets they were joining (i.e. inner join for every join). This means if we go to the source data and do left joins instead, we might get better results. See footnotes on data sources [here](https://www.ibo.nyc.gov/content/publications/2025-march-barriers-to-learning-age-accessibility-space-usage-and-air-conditioning-in-nyc-school-buildings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc306d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: go back and get the original sources of all the data in IBO dataset to see if we can get better coverage.\n",
    "ibo_barriers = pd.read_excel('../data/raw_data/IBO/IBO-barriers-to-learning-data-file.xlsx', sheet_name='DATA')\n",
    "# NOTE: there are only 1309 records in IBO data. \n",
    "print(\"Pct match from IBO to master_schools:\", ibo_barriers['building_code'].isin(master_schools['Bldg_Code']).sum() / len(ibo_barriers))\n",
    "\n",
    "ibo_barriers['central_ac'] = ibo_barriers['central_ac'].map({'Y': 1, 'N': 0})\n",
    "\n",
    "ibo_cols_of_interest = [\n",
    "    'building_code',\n",
    "    'building_ownership_description',\n",
    "    'yearbuilt',\n",
    "    'age',\n",
    "    'bap_rating',\n",
    "    'Accessibility_Description',\n",
    "]\n",
    "ibo_barriers = ibo_barriers[ibo_cols_of_interest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbbf3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_schools = master_schools.merge(ibo_barriers, left_on='Bldg_Code', right_on='building_code', how='left').drop(columns=['building_code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10425d9",
   "metadata": {},
   "source": [
    "## Join Solar-Readiness Data\n",
    "\n",
    "This DCAS dataset comes from [here](https://data.cityofnewyork.us/City-Government/City-of-New-York-Municipal-Solar-Readiness-Assessm/cfz5-6fvh/about_data) and shows which buildings with 10K+ sqft have been assessed for solar-readiness and if they are solar-ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c46043",
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_readiness = pd.read_csv('../data/processed_data/solar_readiness_assessment_doe_buildings.csv')\n",
    "# Rename columns for clarity\n",
    "solar_readiness.rename(columns={\n",
    "    'Site': 'Solar_Site',\n",
    "    'Status': 'Solar_Status',\n",
    "    'Year of Report': 'Solar_Year_of_Report',\n",
    "}, inplace=True)\n",
    "\n",
    "# Join solar readiness data\n",
    "master_schools = master_schools.merge(solar_readiness, left_on='Bldg_Code', right_on='Solar_Site', how='left')#.drop(columns=['Solar_Site'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b989a321",
   "metadata": {},
   "source": [
    "## Join City Council Districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb13bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "council_districts = gpd.read_file('../data/processed_data/city_council_districts.geojson').to_crs(master_schools.crs)\n",
    "master_schools = gpd.sjoin(master_schools, council_districts, how='left', predicate='within').drop(columns=['index_right', 'BOROUGH', 'Shape_Leng', 'Shape_Area'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c912c43",
   "metadata": {},
   "source": [
    "# Export Joined Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaf2529",
   "metadata": {},
   "source": [
    "## Shorten Columns for Shapefile Limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e3c455",
   "metadata": {},
   "outputs": [],
   "source": [
    "shortened_cols = {\n",
    "    # DAC columns\n",
    "    'dac_designation': 'in_dac',\n",
    "    'combined_score': 'comb_score',\n",
    "    'percentile_rank_combined_nyc': 'pctl_comb',\n",
    "    'burden_score': 'burd_score',\n",
    "    'burden_score_percentile': 'pctl_burd',\n",
    "    'vulnerability_score': 'vuln_score',\n",
    "    'vulnerability_score_percentile': 'pctl_vuln',\n",
    "    # Primary results columns\n",
    "    'ZohranFirstRoundFrac': 'ZohrPrimR1',\n",
    "    # IBO columns\n",
    "    'age': 'Bldg_Age',\n",
    "    'building_ownership_description': 'Bldg_Owner',\n",
    "    'Accessibility_Description': 'Accessible',\n",
    "    # Council District columns\n",
    "    'NAME': 'CouncName',\n",
    "    'POLITICAL PARTY': 'CouncParty',\n",
    "    'DISTRICT OFFICE ADDRESS': 'CouncAddr',\n",
    "    'DISTRICT OFFICE PHONE': 'CouncPhone',\n",
    "    # Capacity Utilization columns\n",
    "    'Bldg_Enroll': 'Bldg_Enrl',\n",
    "    # Solar-Readiness columns\n",
    "    'Solar_Status': 'Sol_Stat',\n",
    "    'Solar_Year_of_Report': \"Sol_Yr_Rep\",\n",
    "    'Installed or Estimated Capacity': \"Sol_Capac\",\n",
    "    'Solar-Readiness Assessment': 'Sol_Rd_Ass',\n",
    "    'Percentage of Max Peak Demand': 'SolPctDemd',\n",
    "    'Estimated Annual Production': 'SolEstProd',\n",
    "    'Percentage of Annual Electricity Consumption': 'SolPctCnsm',\n",
    "    'Estimated Annual Emissions Reduction': 'SolEmisRed',\n",
    "    'Estimated Annual Energy Savings': 'SolEnrgSav',\n",
    "    'Upfront Project Cost': 'Sol_Cost',\n",
    "    'Total Gross Square Footage': 'SolTotSqFt',\n",
    "    'Roof Condition': 'Sol_Rf_Con',\n",
    "    'Roof Age': 'Sol_Rf_Age',\n",
    "    'Other Sustaibility Projects': 'Sol_Ot_Prj',\n",
    "    }\n",
    "\n",
    "\n",
    "# test if cols are correct length\n",
    "for col in master_schools.rename(columns=shortened_cols).columns:\n",
    "    if len(col) > 10:\n",
    "        print(f\"{col} too long: currently {len(col)} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2691eff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for Shapefile\n",
    "master_schools = master_schools.rename(columns=shortened_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ce64ba",
   "metadata": {},
   "source": [
    "## Export to Shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a84572a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "# Save shapefile first\n",
    "shp_path = '../data/processed_data/master_schools.shp'\n",
    "master_schools.sort_values('Loc_Code').to_file(\n",
    "    shp_path,\n",
    "    driver='ESRI Shapefile'\n",
    ")\n",
    "\n",
    "# Create zip file with all shapefile components\n",
    "zip_path = '../data/processed_data/master_schools.zip'\n",
    "base_name = '../data/processed_data/master_schools'\n",
    "\n",
    "# Shapefile extensions to include\n",
    "extensions = ['.shp', '.shx', '.dbf', '.prj', '.cpg']\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for ext in extensions:\n",
    "        file_path = base_name + ext\n",
    "        if os.path.exists(file_path):\n",
    "            # Add file to zip with just the filename (no path)\n",
    "            zipf.write(file_path, os.path.basename(file_path))\n",
    "            print(f\"Added {os.path.basename(file_path)} to zip\")\n",
    "\n",
    "print(f\"Shapefile saved as zip: {zip_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c089bae7",
   "metadata": {},
   "source": [
    "## Export to GeoJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d2009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_schools.sort_values('Loc_Code').to_file(\n",
    "    '../data/processed_data/master_schools.geojson', driver='GeoJSON'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv) zohran-ghs-dashboard",
   "language": "python",
   "name": "zohran-ghs-dashboard"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
