{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c17a0f7",
   "metadata": {},
   "source": [
    "# School Locations Processing\n",
    "We have 3 different files with school location information, and each file has slightly different contents. Need to compare contents and resolve what our final/true list of geo-locatable schools is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8acf19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ea58fa",
   "metadata": {},
   "source": [
    "## Load Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a3c5a9",
   "metadata": {},
   "source": [
    "### School Point Locations\n",
    "Data source: https://data.cityofnewyork.us/Education/School-Point-Locations/jfju-ynrr/about_data\n",
    "\n",
    "Last updated: November 26, 2024\n",
    "\n",
    "Annoyingly, the data dictionary on the above linked page doesn't match the data itself, so we're left to guess on the meaning of some of these fields. Also, the description on the above linked page says this data contains Address, Principal, and Principal contact info, but that isn't in here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "111c3f2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "DataSourceError",
     "evalue": "../data/raw_data/DOE/School Locations/School Point Locations/SchoolPoints_APS_2024_08_28/SchoolPoints_APS_2024_08_28.shp: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDataSourceError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m school_points_gdf = \u001b[43mgpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../data/raw_data/DOE/School Locations/School Point Locations/SchoolPoints_APS_2024_08_28/SchoolPoints_APS_2024_08_28.shp\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m school_points_gdf.rename(columns={\u001b[33m'\u001b[39m\u001b[33mLocation_C\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mLocation Code\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mName\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mLocation Name\u001b[39m\u001b[33m'\u001b[39m}, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      3\u001b[39m school_points_gdf\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/geopandas/io/file.py:316\u001b[39m, in \u001b[36m_read_file\u001b[39m\u001b[34m(filename, bbox, mask, columns, rows, engine, **kwargs)\u001b[39m\n\u001b[32m    313\u001b[39m             filename = response.read()\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mpyogrio\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read_file_pyogrio\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mfiona\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    321\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pd.api.types.is_file_like(filename):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/geopandas/io/file.py:576\u001b[39m, in \u001b[36m_read_file_pyogrio\u001b[39m\u001b[34m(path_or_bytes, bbox, mask, rows, **kwargs)\u001b[39m\n\u001b[32m    567\u001b[39m     warnings.warn(\n\u001b[32m    568\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe \u001b[39m\u001b[33m'\u001b[39m\u001b[33minclude_fields\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mignore_fields\u001b[39m\u001b[33m'\u001b[39m\u001b[33m keywords are deprecated, and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    569\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwill be removed in a future release. You can use the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m'\u001b[39m\u001b[33m keyword \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    572\u001b[39m         stacklevel=\u001b[32m3\u001b[39m,\n\u001b[32m    573\u001b[39m     )\n\u001b[32m    574\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m] = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33minclude_fields\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpyogrio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pyogrio/geopandas.py:275\u001b[39m, in \u001b[36mread_dataframe\u001b[39m\u001b[34m(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, fid_as_index, use_arrow, on_invalid, arrow_to_pandas_kwargs, **kwargs)\u001b[39m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_arrow:\n\u001b[32m    271\u001b[39m     \u001b[38;5;66;03m# For arrow, datetimes are read as is.\u001b[39;00m\n\u001b[32m    272\u001b[39m     \u001b[38;5;66;03m# For numpy IO, datetimes are read as string values to preserve timezone info\u001b[39;00m\n\u001b[32m    273\u001b[39m     \u001b[38;5;66;03m# as numpy does not directly support timezones.\u001b[39;00m\n\u001b[32m    274\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mdatetime_as_string\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m result = \u001b[43mread_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m    \u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgdal_force_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m    \u001b[49m\u001b[43msql\u001b[49m\u001b[43m=\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m    \u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[43m=\u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_fids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfid_as_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_arrow:\n\u001b[32m    295\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpa\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pyogrio/raw.py:198\u001b[39m, in \u001b[36mread\u001b[39m\u001b[34m(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, return_fids, datetime_as_string, **kwargs)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Read OGR data source into numpy arrays.\u001b[39;00m\n\u001b[32m     60\u001b[39m \n\u001b[32m     61\u001b[39m \u001b[33;03mIMPORTANT: non-linear geometry types (e.g., MultiSurface) are converted\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    194\u001b[39m \n\u001b[32m    195\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    196\u001b[39m dataset_kwargs = _preprocess_options_key_value(kwargs) \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mogr_read\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_vsi_path_or_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_mask_to_wkb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m    \u001b[49m\u001b[43msql\u001b[49m\u001b[43m=\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[43m=\u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_fids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_fids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatetime_as_string\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdatetime_as_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpyogrio/_io.pyx:1313\u001b[39m, in \u001b[36mpyogrio._io.ogr_read\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpyogrio/_io.pyx:232\u001b[39m, in \u001b[36mpyogrio._io.ogr_open\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mDataSourceError\u001b[39m: ../data/raw_data/DOE/School Locations/School Point Locations/SchoolPoints_APS_2024_08_28/SchoolPoints_APS_2024_08_28.shp: No such file or directory"
     ]
    }
   ],
   "source": [
    "school_points_gdf = gpd.read_file('../data/raw_data/DOE/School Locations/School Point Locations/SchoolPoints_APS_2024_08_28/SchoolPoints_APS_2024_08_28.shp')\n",
    "school_points_gdf.rename(columns={'Location_C': 'Location Code', 'Name': 'Location Name'}, inplace=True)\n",
    "school_points_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfb28c2",
   "metadata": {},
   "source": [
    "### LCGMS\n",
    "Last updated: November 26, 2024\n",
    "\n",
    "This data has more robust fields in it related to grades, address, open date, principal contact info, etc. But there is a discrepancy in the records included in the geocoded vs. non-geocoded files. Not sure yet if there are any other discrepancies between these two files but need to figure that out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd7eea9",
   "metadata": {},
   "source": [
    "#### Non-geocoded LCGMS data\n",
    "Source: https://www.nycenet.edu/PublicApps/LCGMS.aspx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30047234",
   "metadata": {},
   "outputs": [],
   "source": [
    "lcgms_df = pd.read_excel('../data/raw_data/DOE/School Locations/LCGMS/LCGMS_SchoolData_20250806_0112.xlsx', dtype=str, engine='openpyxl')\n",
    "lcgms_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2be48c9",
   "metadata": {},
   "source": [
    "#### Geocoded LCGMS data\n",
    "Source: https://data.cityofnewyork.us/Education/NYC-DOE-Public-School-Location-Information/3bkj-34v2/about_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bd4df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lcgms_geocoded_df = pd.read_csv('../data/raw_data/DOE/School Locations/LCGMS/LCGMS_SchoolData_additional_geocoded_fields_added_.csv', encoding='latin-1')\n",
    "lcgms_geocoded_df\n",
    "# lcgms_geocoded_gdf = gpd.GeoDataFrame(lcgms_geocoded_df, geometry=gpd.GeoSeries.from_xy(lcgms_geocoded_df['lon'], lcgms_geocoded_df['lat']), crs=4326)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f2d399",
   "metadata": {},
   "source": [
    "## Investigate Discrepancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d5f20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: need to somehow summarize for our non-technical folks how/that there are discrepances between these datasets so they can maybe run those down by emailing DOE officials or something. Wouldn't want us suggesting fake schools to the Z campaign or something weird like that.\n",
    "# Maybe they can also just do a manual fact-check on the records that are different between datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60750679",
   "metadata": {},
   "source": [
    "### School Points vs. LCGMS non-geocoded\n",
    "\n",
    "The non-geocoded LCGMS data seems like it is the more \"official\" data compared to School Points and geocoded LCGMS, due to it being the spreadsheet downloaded from following links on the official LCGMS page [here](https://infohub.nyced.org/in-our-schools/operations/lcgms) rather than being sort of a sneaky extra dataset included on NYC open data (i.e. the geocoded one) or a poorly documented Shapefile on NYC Open Data. So, I think it makes sense to trust/prefer the non-geocoded LCGMS data over the geocoded data and attempt to map the non-geocoded LCGMS data somehow. The easiest way to do that would be to attach it to the school points layer, but we need to figure out how doable that is first (i.e. discrepancies in that potential join)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c50801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show columns that are in both school_points_gdf and lcgms_df\n",
    "school_points_gdf.columns.intersection(lcgms_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece1075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show instances where Location Name is different between school_points_gdf and lcgms_df\n",
    "school_points_gdf[['Location Code', 'Location Name']].merge(\n",
    "    lcgms_df[['Location Code', 'Location Name']],\n",
    "    on='Location Code',\n",
    "    how='inner',\n",
    "    suffixes=('_school', '_lcgms')\n",
    ").query('`Location Name_school` != `Location Name_lcgms`')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3007e9",
   "metadata": {},
   "source": [
    "#### How many LCGMS records are NOT in School Points? (i.e. `set(LCGMS).difference(set(School Points))`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5dd8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show records that are NOT in school points but ARE in LCGMS\n",
    "lcgms_records_missing_from_school_points = school_points_gdf[['Location Code', 'Location Name']].merge(\n",
    "    lcgms_df[['Location Code', 'Location Name']],\n",
    "    on='Location Code',\n",
    "    how='outer',\n",
    "    suffixes=('_school_points', '_lcgms'),\n",
    "    indicator=True\n",
    ").query('_merge == \"right_only\"')\n",
    "print(\"Total records in LCGMS that are NOT in School Points:\", len(lcgms_records_missing_from_school_points))\n",
    "lcgms_records_missing_from_school_points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5496da75",
   "metadata": {},
   "source": [
    "Check if we can find the LCGMS records that are missing from School Points in geocoded LCGMS instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d182d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Welp, only 1 of the LCGMS records that are missing from school points are actually in the geocoded LCGMS data, but the lat/lon for that record is missing in geocoded LCGMS as well. So we won't have a use for geocoded LCGMS.\n",
    "# TODO: have someone on our team research these 17 schools we can't map\n",
    "lcgms_records_missing_from_school_points.drop(columns=['_merge']).merge(\n",
    "    lcgms_geocoded_df[['Location Code', 'Location Name', 'Latitude', 'Longitude']],\n",
    "    on='Location Code',\n",
    "    how='left',\n",
    "    suffixes=('_missing_from_school_points', '_geocoded'),\n",
    "    indicator=True\n",
    ").query('_merge == \"both\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3982e3d4",
   "metadata": {},
   "source": [
    "#### How many School Points records are NOT in LCGMS? (i.e. `set(School Points).difference(set(LCGMS))`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4219f0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "school_points_records_missing_from_lcgms = school_points_gdf[['Location Code', 'Location Name']].merge(\n",
    "    lcgms_geocoded_df[['Location Code', 'Location Name']],\n",
    "    on='Location Code',\n",
    "    how='outer',\n",
    "    suffixes=('_school_points', '_lcgms'),\n",
    "    indicator=True\n",
    ").query('_merge == \"left_only\"')\n",
    "print(\"Total records in School Points that are NOT in LCGMS:\", len(school_points_records_missing_from_lcgms))\n",
    "school_points_records_missing_from_lcgms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593fff45",
   "metadata": {},
   "source": [
    "Check if we can find the School Points records that are missing from LCGMS in geocoded LCGMS instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b116771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# phewf - no records from geocoded LCGMS that would have to be added to school points even after joining non-geocoded LCGMS onto school points.\n",
    "school_points_records_missing_from_lcgms.drop(columns=['_merge']).merge(\n",
    "    lcgms_geocoded_df[['Location Code', 'Location Name']],\n",
    "    on='Location Code',\n",
    "    how='left',\n",
    "    suffixes=('_missing_from_lcgms', '_geocoded'),\n",
    "    indicator=True\n",
    ").query('_merge == \"both\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4b740b",
   "metadata": {},
   "source": [
    "#### Are there any records in geocoded LCGMS that are NOT in the joined result of non-geocoded LCGMS + School Points?\n",
    "\n",
    "Thankfully, no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7e1a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show records that are in geocoded LCGMS but NOT in the joined result of non-geocoded LCGMS + School Points\n",
    "school_points_gdf.merge(\n",
    "    lcgms_df[['Location Code', 'Location Name']],\n",
    "    on='Location Code',\n",
    "    how='outer',\n",
    ").merge(\n",
    "    lcgms_geocoded_df[['Location Code', 'Location Name']],\n",
    "    on='Location Code',\n",
    "    how='left',\n",
    "    suffixes=('', '_geocoded'),\n",
    "    indicator=True\n",
    ").query('_merge == \"right_only\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e7fe2c",
   "metadata": {},
   "source": [
    "### LCGMS non-geocoded vs. LCGMS geocoded\n",
    "\n",
    "The TL;DR here is that the delta between these two datasets is really befuddling, especially since it seems the geocoded one should be the exact same as the non-geocoded one but for additional lat/lon fields. I don't know the rhyme nor reason for these discrepancies, and so I think we should just prefer/trust the more official-looking one wherever possible, which is the non-geocoded LCGMS.\n",
    "\n",
    "Would love for the civil servants who made these datasets to explain why they are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2f6732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE from LCGMS data dict: \"LOCATION CODE: a unique identifier that can include schools, administrative offices, learning communities, etc.  When the Learning_Community_Name = ‘School’, the Location_Code is a combination of the borough code and the school number.\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c551e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show fields that are in lcgms_geocoded_df but NOT in lcgms_df\n",
    "lcgms_geocoded_df.columns.difference(lcgms_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cf022e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show fields that are in lcgms_df but NOT in lcgms_geocoded_df\n",
    "lcgms_df.columns.difference(lcgms_geocoded_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaeca5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare data in shared columns between lcgms_df and lcgms_geocoded_df\n",
    "shared_columns = set(lcgms_df.columns).intersection(set(lcgms_geocoded_df.columns))\n",
    "print(f\"Shared columns: {sorted(shared_columns)}\")\n",
    "\n",
    "# Merge the dataframes on Location Code to compare shared columns\n",
    "comparison_df = lcgms_df.merge(\n",
    "    lcgms_geocoded_df, \n",
    "    on='Location Code', \n",
    "    how='inner', \n",
    "    suffixes=('_non_geocoded', '_geocoded')\n",
    ")\n",
    "\n",
    "# Check for differences in each shared column (excluding Location Code which is the join key)\n",
    "shared_data_columns = [col for col in shared_columns if col != 'Location Code']\n",
    "differences_summary = {}\n",
    "\n",
    "for col in shared_data_columns:\n",
    "    col_non_geo = f\"{col}_non_geocoded\"\n",
    "    col_geo = f\"{col}_geocoded\"\n",
    "    \n",
    "    # Count records where values differ (handling NaN values)\n",
    "    different_mask = (\n",
    "        (comparison_df[col_non_geo].fillna('') != comparison_df[col_geo].fillna('')) |\n",
    "        (comparison_df[col_non_geo].isna() != comparison_df[col_geo].isna())\n",
    "    )\n",
    "    \n",
    "    num_differences = different_mask.sum()\n",
    "    differences_summary[col] = num_differences\n",
    "    \n",
    "    if num_differences > 0:\n",
    "        print(f\"\\n{col}: {num_differences} differences found\")\n",
    "        # Show first few examples of differences\n",
    "        diff_examples = comparison_df[different_mask][['Location Code', col_non_geo, col_geo]].head()\n",
    "        print(diff_examples)\n",
    "\n",
    "print(f\"\\nSummary of differences:\")\n",
    "for col, count in differences_summary.items():\n",
    "    print(f\"{col}: {count} differences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0308e888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show records where Location Code matches but Location Name does not match\n",
    "lcgms_df[['Location Code', 'Location Name']].merge(\n",
    "    lcgms_geocoded_df[['Location Code', 'Location Name']], \n",
    "    on='Location Code', \n",
    "    how='inner', \n",
    "    suffixes=('_non-geocoded', '_geocoded')\n",
    ").query('`Location Name_non-geocoded` != `Location Name_geocoded`')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54626312",
   "metadata": {},
   "source": [
    "## Join Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67ded85",
   "metadata": {},
   "source": [
    "### Outer Join LCGMS with School Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a770f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our final school points data (at least for now), join LCGMS onto school points\n",
    "school_points_with_lcgms = school_points_gdf.merge(\n",
    "    # Just going to use Location Name from School Points bc above check showed that there's minimal differences there.\n",
    "    lcgms_df.drop(columns=['Location Name']),\n",
    "    on='Location Code',\n",
    "    how='outer',\n",
    "    indicator=True\n",
    ")\n",
    "\n",
    "# Keep a column that indicates whether the record is missing from LCGMS or not so we can select just LCGMS records if we find out that the school points data is outdated or inaccurate compared to LCGMS.\n",
    "school_points_with_lcgms.rename(columns={'_merge': 'in_LCGMS'}, inplace=True)\n",
    "school_points_with_lcgms['in_LCGMS'] = school_points_with_lcgms['in_LCGMS'].str.contains('both|right_only')\n",
    "\n",
    "school_points_with_lcgms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7043ece9",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5087177",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "school_points_with_lcgms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1712cc92",
   "metadata": {},
   "source": [
    "## Cleaning up specific fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038f7d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: not sure why we have disagreements between Building Code and Building_C\n",
    "school_points_with_lcgms[\n",
    "    (school_points_with_lcgms['Building_C'] != school_points_with_lcgms['Building Code'])\n",
    "    # & school_points_with_lcgms['Building Code'].notna()\n",
    "    # & school_points_with_lcgms['Building_C'].notna()\n",
    "][['Location Code', 'Location Name', 'Building Code', 'Building_C', 'in_LCGMS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fb9dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, we're going to keep the Building Code from LCGMS unless NaN\n",
    "school_points_with_lcgms['Building Code'] = school_points_with_lcgms['Building Code'\n",
    "                                                                     ].fillna(school_points_with_lcgms['Building_C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5006b3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: need to go through all the LCGMS columns and figure out which ones we can drop\n",
    "# Drop unnecessary columns\n",
    "cols_to_drop = [\n",
    "    'Geographic', # This is from School Points and I have no idea what it means.\n",
    "    'Building_C',  # This is the Building Code from School Point, which is duplicate\n",
    "]\n",
    "school_points_with_lcgms.drop(columns=cols_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcd7f9a",
   "metadata": {},
   "source": [
    "# Deal with Duplicate geometries\n",
    "\n",
    "Basically half of the points are from duplicate locations. Wondering if we can bring in the buildings data to get more precise locations and fix this. Otherwise, we don't end up seeing half of the points on the map because they're on top of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3babd621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: is this still true if we use the lat/on instead of native shapefile geometry?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cba9230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok at least with these 3 examples, they all appear to just be different schools that \n",
    "# share an address:\n",
    "#    - CSI High School for International Studies\n",
    "#    - Gaynor McCown Expeditionary Learning School\n",
    "#    - Marsh Avenue School for Expeditionary Learning\n",
    "school_points_with_lcgms[\n",
    "    (school_points_with_lcgms.duplicated(subset=['geometry'], keep=False))\n",
    "    & school_points_with_lcgms['geometry'].notna()\n",
    "    ].sort_values(by='geometry')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a275b2",
   "metadata": {},
   "source": [
    "## Load OTI Buildings Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473f4869",
   "metadata": {},
   "outputs": [],
   "source": [
    "oti_buildings_gdf = gpd.read_file('../data/raw_data/OTI/BUILDING_20250911.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621b3034",
   "metadata": {},
   "outputs": [],
   "source": [
    "school_points_with_lcgms.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b88b759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: what is the difference between base_bbl and mappluto_bbl?\n",
    "oti_buildings_gdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204ba298",
   "metadata": {},
   "outputs": [],
   "source": [
    "school_points_with_lcgms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099f9cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "oti_buildings_gdf['base_bbl'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc08d4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "school_points_with_lcgms.loc[0, 'Borough Block Lot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be507a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "oti_buildings_gdf[oti_buildings_gdf['base_bbl'] == '3007550022'].explore()#.explore(tiles='CartoDB positron',\n",
    "                # popup=['Location Name', 'Community District', 'Council District',\n",
    "                #        'Principal Name', 'Principal Title', 'Principal Phone Number'],\n",
    "                # tooltip=['Location Name'],  # Show on hover\n",
    "                # legend=True,\n",
    "                # style_kwds={'fillOpacity': 0.7, 'weight': 1}\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3da5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 94% of school points have at least one match in OTI buildings based on BBL. The problem is many have multiple matches.\n",
    "print('pct of school points with matching BBL in OTI buildings:', school_points_with_lcgms['Borough Block Lot'].isin(oti_buildings_gdf['base_bbl']).sum() / len(school_points_with_lcgms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04764b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "school_points_with_lcgms['Location Code'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2368037f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show number of unique geometries in school_points_with_lcgms\n",
    "school_points_with_lcgms['geometry'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4223bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show number of unique geometries in OTI buildings that match to school points based on BBL, when dropping dupes by keeping largest matched building\n",
    "# hmmm ok so this is actually worse than LCGMS alone...1320 unique geometries here, but 1381 before with just LCGMS\n",
    "school_points_with_lcgms.merge(\n",
    "    oti_buildings_gdf, \n",
    "    left_on='Borough Block Lot', \n",
    "    right_on='base_bbl', \n",
    "    how='left', \n",
    "    suffixes=['_lcgms', '_oti']\n",
    ").sort_values(\n",
    "    by='shape_area',\n",
    "    # keep the largest building match\n",
    "    ascending=False\n",
    "    ).drop_duplicates(\n",
    "        subset=['Location Code'],\n",
    "        keep='first')['geometry_oti'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0502aa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to drop dupes on Location Code, choosing a unique geometry where possible.\n",
    "# So if a duped Location Code has a geometry that isn't used by any other Location Code, prefer that one.\n",
    "# If none of the dupes have a unique geometry, just keep the first one.\n",
    "school_points_with_lcgms.merge(\n",
    "    oti_buildings_gdf,\n",
    "    left_on='Borough Block Lot',\n",
    "    right_on='base_bbl',\n",
    "    how='left',\n",
    "    suffixes=['_lcgms', '_oti']\n",
    ").drop_duplicates(\n",
    "        subset=['Location Code'],\n",
    "        keep='first')['geometry_oti'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26acc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to figure out what I can drop from OTI. Then figure out how I join to school points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37760e0",
   "metadata": {},
   "source": [
    "# Sanity Check: Visualize Data with Geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3b602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "school_points_with_lcgms.explore(tiles='CartoDB positron',\n",
    "                popup=['Location Name', 'Community District', 'Council District',\n",
    "                       'Principal Name', 'Principal Title', 'Principal Phone Number'],\n",
    "                tooltip=['Location Name'],  # Show on hover\n",
    "                legend=True,\n",
    "                style_kwds={'fillOpacity': 0.7, 'weight': 1}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faddc1f",
   "metadata": {},
   "source": [
    "# Export to `processed_data` folder as GeoJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192e4e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "school_points_with_lcgms.to_file(\n",
    "    '../data/processed_data/school_points_with_lcgms.geojson',\n",
    "      driver='GeoJSON'\n",
    "      )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
